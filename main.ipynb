{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Classes: 100%|██████████| 20/20 [00:00<00:00, 385.24it/s]\n",
      "Loading Classes: 100%|██████████| 20/20 [00:00<00:00, 984.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset Statistics:\n",
      "Training samples: 14400\n",
      "Validation samples: 3600\n",
      "Test samples: 6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating stats:   0%|          | 0/450 [00:00<?, ?it/s]Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/harshithkethireddy/opt/miniconda3/lib/python3.9/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/harshithkethireddy/opt/miniconda3/lib/python3.9/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'HandGestureDataset' on <module '__main__' (built-in)>\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Hand Gesture Recognition - PyTorch Preprocessing Pipeline\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# ======================\n",
    "# Configuration\n",
    "# ======================\n",
    "class Config:\n",
    "    # Directory configuration\n",
    "    base_dir = \"/Users/harshithkethireddy/Documents/intelligent_data\"\n",
    "    train_dir = os.path.join(base_dir, \"train/train\")\n",
    "    test_dir = os.path.join(base_dir, \"test/test\")\n",
    "    \n",
    "    # Image parameters\n",
    "    img_size = (224, 224)\n",
    "    num_classes = 20\n",
    "    batch_size = 32\n",
    "    num_workers = 4\n",
    "    validation_ratio = 0.2\n",
    "    random_seed = 42\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(Config.random_seed)\n",
    "np.random.seed(Config.random_seed)\n",
    "\n",
    "# ======================\n",
    "# Custom Dataset Class\n",
    "# ======================\n",
    "class HandGestureDataset(Dataset):\n",
    "    def __init__(self, root_dir, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.transform = transform\n",
    "        self.classes = sorted(os.listdir(root_dir))\n",
    "        self.image_paths = []\n",
    "        self.labels = []\n",
    "        \n",
    "        # Load image paths and labels\n",
    "        for label, class_name in enumerate(tqdm(self.classes, desc=\"Loading Classes\")):\n",
    "            class_dir = os.path.join(root_dir, class_name)\n",
    "            for img_name in os.listdir(class_dir):\n",
    "                self.image_paths.append(os.path.join(class_dir, img_name))\n",
    "                self.labels.append(label)\n",
    "                \n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load image\n",
    "        image = Image.open(img_path).convert('L')  # Convert to grayscale\n",
    "        \n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# ======================\n",
    "# Transformation Pipelines\n",
    "# ======================\n",
    "# Base transformations\n",
    "base_transform = transforms.Compose([\n",
    "    transforms.Resize(Config.img_size),\n",
    "    transforms.ToTensor(),  # Converts to [0,1] and CxHxW format\n",
    "])\n",
    "\n",
    "# Augmentation transformations\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(Config.img_size),\n",
    "    transforms.RandomRotation(15),  # ±15 degrees\n",
    "    transforms.RandomAffine(\n",
    "        degrees=0, \n",
    "        translate=(0.1, 0.1),  # ±10% translation\n",
    "        scale=(0.9, 1.1)  # ±10% zoom\n",
    "    ),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize to [-1,1]\n",
    "])\n",
    "\n",
    "# Validation/test transformations\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Resize(Config.img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])\n",
    "])\n",
    "\n",
    "# ======================\n",
    "# Data Loading Functions\n",
    "# ======================\n",
    "def create_datasets():\n",
    "    # Create base datasets\n",
    "    full_train_dataset = HandGestureDataset(\n",
    "        Config.train_dir,\n",
    "        transform=base_transform  # Apply only basic transforms initially\n",
    "    )\n",
    "    \n",
    "    test_dataset = HandGestureDataset(\n",
    "        Config.test_dir,\n",
    "        transform=base_transform\n",
    "    )\n",
    "    \n",
    "    # Split training into train/validation\n",
    "    train_size = int((1 - Config.validation_ratio) * len(full_train_dataset))\n",
    "    val_size = len(full_train_dataset) - train_size\n",
    "    \n",
    "    train_dataset, val_dataset = random_split(\n",
    "        full_train_dataset,\n",
    "        [train_size, val_size],\n",
    "        generator=torch.Generator().manual_seed(Config.random_seed)\n",
    "    )\n",
    "    \n",
    "    # Apply proper transforms to each subset\n",
    "    train_dataset.dataset.transform = train_transform\n",
    "    val_dataset.dataset.transform = val_test_transform\n",
    "    test_dataset.transform = val_test_transform\n",
    "    \n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# ======================\n",
    "# Data Visualization\n",
    "# ======================\n",
    "def visualize_batch(dataloader, num_images=8):\n",
    "    dataiter = iter(dataloader)\n",
    "    images, labels = next(dataiter)\n",
    "    \n",
    "    # Convert images back to 0-1 range for display\n",
    "    inverse_normalize = transforms.Normalize(\n",
    "        mean=[-0.5/0.5],  # Reverse normalization\n",
    "        std=[1/0.5]\n",
    "    )\n",
    "    images = inverse_normalize(images)\n",
    "    \n",
    "    fig = plt.figure(figsize=(12, 6))\n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(2, 4, i+1)\n",
    "        img = images[i].squeeze().numpy()\n",
    "        plt.imshow(img, cmap='gray')\n",
    "        plt.title(f\"Label: {labels[i].item()}\")\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# ======================\n",
    "# Data Loader Creation\n",
    "# ======================\n",
    "def create_dataloaders(train_dataset, val_dataset, test_dataset):\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        shuffle=False,\n",
    "        num_workers=Config.num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_loader, val_loader, test_loader\n",
    "\n",
    "# ======================\n",
    "# Dataset Statistics\n",
    "# ======================\n",
    "def calculate_dataset_stats(dataset):\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=Config.batch_size,\n",
    "        num_workers=Config.num_workers\n",
    "    )\n",
    "    \n",
    "    mean = 0.\n",
    "    std = 0.\n",
    "    nb_samples = 0.\n",
    "    \n",
    "    for images, _ in tqdm(loader, desc=\"Calculating stats\"):\n",
    "        batch_samples = images.size(0)\n",
    "        images = images.view(batch_samples, images.size(1), -1)\n",
    "        mean += images.mean(2).sum(0)\n",
    "        std += images.std(2).sum(0)\n",
    "        nb_samples += batch_samples\n",
    "        \n",
    "    mean /= nb_samples\n",
    "    std /= nb_samples\n",
    "    \n",
    "    return mean, std\n",
    "\n",
    "# ======================\n",
    "# Main Execution\n",
    "# ======================\n",
    "if __name__ == \"__main__\":\n",
    "    # Create datasets\n",
    "    train_dataset, val_dataset, test_dataset = create_datasets()\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, val_loader, test_loader = create_dataloaders(\n",
    "        train_dataset, val_dataset, test_dataset\n",
    "    )\n",
    "    \n",
    "    # Print dataset statistics\n",
    "    print(\"\\nDataset Statistics:\")\n",
    "    print(f\"Training samples: {len(train_dataset)}\")\n",
    "    print(f\"Validation samples: {len(val_dataset)}\")\n",
    "    print(f\"Test samples: {len(test_dataset)}\")\n",
    "    \n",
    "    # Calculate and print mean/std\n",
    "    train_mean, train_std = calculate_dataset_stats(train_dataset)\n",
    "    print(f\"\\nTraining set - Mean: {train_mean.item():.4f}, Std: {train_std.item():.4f}\")\n",
    "    \n",
    "    # Visualize training batch\n",
    "    print(\"\\nVisualizing training batch...\")\n",
    "    visualize_batch(train_loader)\n",
    "    \n",
    "    # Example usage in training loop:\n",
    "    print(\"\\nSample training loop structure:\")\n",
    "    for epoch in range(2):  # Demo with 2 epochs\n",
    "        print(f\"\\nEpoch {epoch+1}\")\n",
    "        progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "        for images, labels in progress_bar:\n",
    "            # Your training code here\n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
